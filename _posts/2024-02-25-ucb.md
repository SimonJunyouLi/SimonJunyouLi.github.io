---
title: Uppder Confidence Bound
subtitle: I derive the Hoeffding's Bound and analyze the UCB1 algorithm.
authors: Simon Li
layout: default
date: 2024-02-25
keywords: machine learning, reinforcement learning
tags: ml rl
published: true
---

# Upper Confidence Bound

To understand UCB1, we would need to use Hoeffding’s Inequality.

## Hoeffding’s Inequality

***Theorem:** Let {% katexmm %}$\Theta_1, \dots, \Theta_N${% endkatexmm %} be a sequence of i.i.d. random variables with mean {% katexmm %}$\mathbb{E}[\Theta]${% endkatexmm %} and range {% katexmm %}$[a,b]${% endkatexmm %}. Then, for any {% katexmm %}$\epsilon > 0${% endkatexmm %}, we have that* 

{% katexmm %}
$$
\Pr\left[ \left| \frac{1}{N} \sum^N_{n=1} \Theta_N - \mathbb{E}[\Theta] \right| \geq \epsilon \right] \leq 2e^{-2N\epsilon^2 / (b-a)^2}
$$
{% endkatexmm %}

To prove this, we would need to use ***Hoeffding’s Lemma***

***Hoeffding’s Lemma:** For any random variable {% katexmm %}$X${% endkatexmm %}, with {% katexmm %}$\mathbb{E}[X] = 0${% endkatexmm %} and {% katexmm %}$X \in [a,b]${% endkatexmm %}, we have* 

{% katexmm %}
$$
\mathbb{E}[e^{sX}] \leq e^{\frac{1}{8}s^2(b-a)^2} \text{ for any } s \geq 0
$$
{% endkatexmm %}

***Proof of Hoeffding Inequality:***

Assume that {% katexmm %}$\mathbb{E}[\Theta] = 0${% endkatexmm %} and that {% katexmm %}$\Theta_n \in [a,b]${% endkatexmm %}. We will only show that 

{% katexmm %}
$$
\Pr\left[ \frac{1}{N} \sum^N_{n=1} \Theta_n \geq \epsilon \right] \leq e^{-2N\epsilon^2/(b-a)^2}
$$
{% endkatexmm %}

as it is similar to show that  

{% katexmm %}
$$
\Pr\left[ \frac{1}{N} \sum^N_{n=1} \Theta_n \leq -\epsilon \right] \leq e^{-2N\epsilon^2/(b-a)^2}
$$
{% endkatexmm %}

Observe that for any $s \geq 0$, we have that 

{% katexmm %}
$$
\begin{aligned}
\Pr\left[ \frac{1}{N} \sum^N_{n=1} \Theta_n \geq \epsilon \right]  &= \Pr\left[ s\frac{1}{N}\sum^N_{n=1}\Theta_n \geq s\epsilon \right] \\
&= \Pr\left[ e^{s\frac{1}{N}\sum^N_{n=1}\Theta_n} \geq e^{s\epsilon} \right] \\
&\leq \mathbb{E}\left[ e^{s\frac{1}{N}\sum^N_{n=1}\Theta_n} \right] e^{-s\epsilon} && \text{by Markov's Inequality} \\
&=\Pi^N_{n=1}\mathbb{E}\left[e^{\frac{s\Theta_n}{N}}\right]e^{-s\epsilon} && \text{r.v. $\Theta_n$ are ind.} \\
&=\mathbb{E}\left[e^{\frac{s\Theta_n}{N}}\right]^Ne^{-s\epsilon} && \text{r.v. $\Theta_n$ are i.d.} \\
&\leq e^{s^2(b-a)^2/(8N)}e^{-s\epsilon} && \text{by Hoeffding lemma}
\end{aligned}
$$
{% endkatexmm %}

In particular, for {% katexmm %}$s = \frac{4N\epsilon}{(b-a)^2}${% endkatexmm %}, we have that 

{% katexmm %}
$$
\Pr\left[\frac{1}{N}\sum^N_{n=1}\Theta_n \geq \epsilon \right] \leq e^{-2N\epsilon^2/(b-a)^2}
$$
{% endkatexmm %}

as desired. {% katexmm %}$\blacksquare${% endkatexmm %}

## UCB1

The UCB1 is a function that converts a set of average rewards at trial $t$ into a set of decision values, which are then used to determine which bandit machine to play. The gist of the UCB1 algorithm is as follows.

For problems such as multi-armed bandits, we would like to maintain confidence bounds on the reward {% katexmm %}$r(a)${% endkatexmm %} of each action $a$ based on observations

{% katexmm %}
$$
\Pr[LCB(a) < r(a) < UCB(a)] \geq 1 - \delta
$$
{% endkatexmm %}

Using the confidence bound, we can successively eliminate the actions. 

For example, suppose we have two actions {% katexmm %}$a_1${% endkatexmm %} and {% katexmm %}$a_2${% endkatexmm %}. We will alternate playing {% katexmm %}$a_1${% endkatexmm %} and {% katexmm %}$a_2${% endkatexmm %} until {% katexmm %}$UCB_T(a_2) < LCB_T(a_1)${% endkatexmm %} or {% katexmm %}$UCB_T(a_1) < LCB_T(a_2)${% endkatexmm %}.  For our case, assume that we achieved {% katexmm %}$UCB_T(a_2) < LCB_T(a_1)${% endkatexmm %}. Then, we eliminate {% katexmm %}$a_2${% endkatexmm %} since the probability of {% katexmm %}$a_2${% endkatexmm %} being the optimal policy is less than {% katexmm %}$2\delta${% endkatexmm %}, which we can manipulate to be small.

To analyze the efficacy of the UCB1 algorithm, we would like to express its results in terms of the ***cumulative regret*** {% katexmm %}$L_T${% endkatexmm %}, which can be defined as 

{% katexmm %}
$$
L_T = \sum^T_{t=1}l_t = \max_a\sum^T_{t=1}r_t(a) - \sum^T_{t=1}r_t(\pi)
$$
{% endkatexmm %}

where regret $l$ = difference in reward between

- action $a^*$ taken by the best static policy
- action taken by the agent’s policy $\pi$

Let’s consider the ***Hoeffding’s Inequality*** for random {% katexmm %}$X${% endkatexmm %} scaled to {% katexmm %}$[0..1]${% endkatexmm %} with {% katexmm %}$t${% endkatexmm %} samples in total and {% katexmm %}$N_t(a)${% endkatexmm %} samples for actino {% katexmm %}$a${% endkatexmm %}. Then, in order to have thet bound {% katexmm %}$\Pr[Q^* > Q_t + u] \leq \delta${% endkatexmm %}, we need to have 

{% katexmm %}
$$
u(a) = \sqrt{\frac{-\log\delta}{2N_t(a)}} = \sqrt{\frac{2\log t}{N_t(a)}}
$$
{% endkatexmm %}

where the second equality holds for {% katexmm %}$\delta = t^{-4}${% endkatexmm %}. The UCB1 algorithm assumes actual reward is close to the upper bound and selects the best action according to {% katexmm %}$Q_t(a) + u(a)${% endkatexmm %}. 

Then, 

***Theorem** the expected cumulative regret for UCB1 algorithm is* 

{% katexmm %}
$$
\lim_{t\to\infty} L_t \leq \frac{8 \log t}{\sum_a \Delta a}
$$
{% endkatexmm %}

where {% katexmm %}$\Delta a = max_{a^\prime} r(a^\prime) - r(a)${% endkatexmm %}.

Some useful notes: [https://webdocs.cs.ualberta.ca/~games/go/seminar/notes/2007/slides_ucb.pdf](https://webdocs.cs.ualberta.ca/~games/go/seminar/notes/2007/slides_ucb.pdf)