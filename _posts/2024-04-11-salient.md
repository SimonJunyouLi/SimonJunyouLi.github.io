---
title: Leveraging Contextual Information for Effective Entity Salience Detection
subtitle: Salience detection with fine-tuned medium-sized language models.
authors: Rajarshi Bhowmik, Marco Ponza, Atharva Tendle, Anant Gupta, Rebecca Jiang, Xingyu Lu, Qian Zhao, Daniel Preotiuc-Pietro
layout: default
date: 2024-04-11
keywords: machine learning
tags: ml
published: true
---

This paper investigates the efficacy of **medium-sized** language model with a **cross-encoder** style architecture in ***salience detection***. Comprehensive benchmarking results demonstrate that the proposed architecture achieves significant improvement over previous feature engineering approaches.

## Introduction

**Salient entities** are entities that are central to a piece of text, quanlified by either a binary or ordinal rating. Previous work explored heave feature engineering to craft explicit features to cover relevant aspects. This paper studies the effectiveness of *Transformer-based Pre-trained Language Models* (PLMs) for entity salience detection. The proposed method adopts a cross-encoder architecture where the entity or its alias along side its contextual mentions are encoded by a PLM. Then, a classifier uses the contextual representations and optionally positional information to determine the salience score of the target entity.

## Related Work





## Future Work

Utilizing external knowledge for saliance detection.