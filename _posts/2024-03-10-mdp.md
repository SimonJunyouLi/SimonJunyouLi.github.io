---
title: Introduction to Markov Decision Process
subtitle: I provide a brief introduction to MDPs.
authors: Simon Li
layout: default
date: 2024-03-10
keywords: machine learning
tags: ml
published: true
---

## General Markove Decision Process
Markov Decision Process (MDP) can be defined as a tuple

{% katexmm %}$$ (\mathcal{S}, \mathcal{A}, p, \mu, r, \gamma) $${% endkatexmm %}

where 
- {% katexmm %}$\mathcal{S}${% endkatexmm %} is the state space
- {% katexmm %}$\mathcal{A}${% endkatexmm %} is the action space
- {% katexmm %}$p(\cdot|s,a)${% endkatexmm %} is the transition probabilities over the next state given the current state {% katexmm %}$s${% endkatexmm %} and current action {% katexmm %}$a${% endkatexmm %}
- {% katexmm %}$r: \mathcal{S} \times \mathcal{A} \to \R${% endkatexmm %} is the reward function
- {% katexmm %}$\mu : \mathcal S \to [0,1]${% endkatexmm %} is the initial state distribution 
- {% katexmm %}$\gamma${% endkatexmm %} is the discount factor for future rewards

The policy of an agent can be repsented as {% katexmm %}$\pi : \mathcal{S} \times \mathcal{A} \to [0,1]${% endkatexmm %}, which is a mapping from a state to a probability distributino over actions.

## Constrained Markov Decision Process
A *constrained MDP$ inherits the general MDP structure with an addition of constraint function 
{% katexmm %}$$c: \mathcal{S} \times \mathcal{A} \to \R$${% endkatexmm %}
and an episodic constraint threshold {% katexmm %}$\beta${% endkatexmm %}.