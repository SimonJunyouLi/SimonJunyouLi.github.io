---
title: Theorem 1 Proof
authors: Simon Li
layout: default
date: 2024-03-10
keywords: machine learning
tags: ml
published: true
---

## Setting

The alternating procedure is as follows:

{% katexmm %}
$$
\text{Policy Opimization: } \pi^* \coloneqq \argmax_\pi J^\pi_\mu (r) \text{ s.t. } J^\pi_\mu (c) \leq \beta \text{ and } \Pi \leftarrow \Pi \cup\{\pi^*\} ~{}(1) \\
\text{Constraint Adjustment: } c^* \coloneqq \argmax_c \min_{\pi \in \Pi} J^\pi_\mu(c) \text{ s.t. } J^{\pi_E}_\mu (c) \leq \beta ~{} (2)
$$
{% endkatexmm %}

And the theorem 1 is as follows:

***Theorem 1:** Assuming there is a unique policy {% katexmm %}$\pi^E${% endkatexmm %} that achieves {% katexmm %}$J^{\pi^E}_\mu (r)${% endkatexmm %}, the alternation of optimization procedures in (1) and (2) converges to a set of policies {% katexmm %}$\Pi${% endkatexmm %} such that the last policy {% katexmm %}$\pi^*${% endkatexmm %} added to {% katexmm %}$\Pi${% endkatexmm %}  is equivalent to the expert policy {% katexmm %}$\pi^E${% endkatexmm %} in the sense that {% katexmm %}$\pi^*${% endkatexmm %} and {% katexmm %}$\pi^E${% endkatexmm %} generate the same trajectories.*

## Proof of Theorem 1

We wish to prove by contradiction under 3 assumptions:

1. the reward function is non-negative
2. the space of policies is finite
3. there is a unique policy {% katexmm %}$\pi^E${% endkatexmm %} that achieves {% katexmm %}$J^{\pi^E}_\mu (r)${% endkatexmm %}. In order words, no other policy achieves the same expected cumulative reward as {% katexmm %}$\pi^E${% endkatexmm %}

Note: 

- first assumption is not restrictive as we can simply shift the reward function by constant
- second assumption is reasonable as parameters have a finite precision â†’ policies also finite

Let {% katexmm %}$\pi^*${% endkatexmm %} be the optimal policy found and {% katexmm %}$c^*${% endkatexmm %} be the optimal constraint function found 

By (1), {% katexmm %}$J^{\pi^E}_\mu (c^*) \leq \beta${% endkatexmm %}. 

By (2), we also have that 

{% katexmm %}
$$
\begin{align*}
&\max_c \min_{\pi \in \Pi} J^\pi_\mu(c) \text{ subject to } J^{\pi^E}_\mu (c) \leq \beta \\
&= \min_{\pi \in \Pi} J^\pi_\mu (c^*) \\
&\leq \beta
\end{align*}
$$
{% endkatexmm %}


For the sake of contradiction, assume that {% katexmm %}$\pi^*${% endkatexmm %} is not equivalent to {% katexmm %}$\pi^E${% endkatexmm %}. Then, we want to show a contradiction that the objective in Equation (2) is greater than {% katexmm %}$\beta${% endkatexmm %}.

{% katexmm %}
$$
\begin{align*}
& \max_c \min_{\pi \in \Pi} J^\pi_\mu(c) \text{ such that } J^{\pi^E}_\mu (c) \leq \beta && \\
&\geq \min_{\pi \in \Pi} J^\pi_\mu(\hat{c}) && \hat{c}(s,a) = \frac{r(s,a)\beta}{J^{\pi^E}_\mu(r)} \\
&= \min_{\pi \in \Pi} J^\pi_\mu(r)\beta / J^{\pi^E}_\mu(r) && \text{by substituting $\hat{c}$} \\
&> J^{\pi^E}_\mu(r)\beta/J^{\pi^E}_\mu(r) = \beta && \text{since $J^\pi_\mu(r) \geq J^{\pi^E}_\mu(r) ~{} \forall\pi\in\Pi$ by (1)} \\ & && \text{and $J^\pi_\mu(r) \neq J^{\pi^E}_\mu(r)$ by assumption (iii)}
\end{align*}
$$
{% endkatexmm %}

The key step is choosing a specific constraint function {% katexmm %}$\hat{c}${% endkatexmm %}. Intuitively, constraint functions are selected to be parallel to the reward function while {% katexmm %}$J^{\pi^E}_\mu(c) \leq \beta${% endkatexmm %}.

**We know that all policies in {% katexmm %}$\Pi${% endkatexmm %} achieve higher expected cumulative rewards than {% katexmm %}$\pi^E${% endkatexmm %},**