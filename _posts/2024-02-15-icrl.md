---
title: Benchmarking constraint inference in inverse reinforcement learning
subtitle: Introducing an ICRL benchmark in the context of RL application domains.
authors: G Liu, Y Luo, A Gaurav, K Rezaee, P Poupart
layout: default
date: 2024-02-15
keywords: machine learning, reinforcement learning, inverse reinforcement constraint learning
tags: ml rl ircl
published: true
---

## abstract

agents unaware of 

    underlying constraints ← hard to specify mathematically

inverse constrained reinforcement learning (ICRL)

    estimates from expert demonstrations

## introduction

constrained reinforcement learning (CRL)

learns policy under some known or predefined constraints

not realistic in real-world → hard to specify exact constraints

**infer from expert demonstrations**

ICRL

infers a constraint function to approximate constraints respected by expert demonstrations

alternating between updating 

imitating policy

constraint function

<div class='figure'>
    <img src="/image/icrl_arch.png"
         style="width: 60%; display: block; margin: 0 auto;"/>
</div>

<!-- ![Screenshot 2024-02-14 at 2.09.54 AM.png](paper%203%2011714d15e744477ea2c22beec0537e4f/Screenshot_2024-02-14_at_2.09.54_AM.png) -->
contributions:
1. expert demonstrations might violate constraint
2. under stochastic environment
3. muultiple constraints
4. recovering the exact least constraining constraint

## background

### **constrained reinforcement learning**

based on constrained markov decision processes (CMDPs)

to learn policy under cmdp, consider following optimization

**cumulative constraints**

{% katexmm %}
$$
\argmax_\pi \mathbb{E}_{p_\mathcal{R}, p_\mathcal{T}, \pi} \left[ \sum^T_{t=0} \gamma^tr_t \right] + \frac{1}{\beta}\mathcal{H}(\pi) \text{ s.t } \mathbb{E}_{p_{\mathcal{c}_i}, p_\mathcal{T}, \pi} \left[ \sum^T_{t=0} \gamma^t c_i(s_t,a_t) \right] \leq \epsilon_i ~{} \forall i \in [0,I]
$$
{% endkatexmm %}

where {% katexmm %}$\mathcal{H}(\pi)${% endkatexmm %} denotes the policy entropy weighted by {% katexmm %}$\frac{1}{\beta}${% endkatexmm %}

commonly used in **soft** setting → recover from undesirable movement i.e. high cost

discounted additive cost less than the threshold

{% katexmm %}
$$
c_i(s_t,a_t)
$$
{% endkatexmm %}

**trajectory-based constraints** (alternative approach)

defining constraints w/o relying on discounted factor

{% katexmm %}
$$
\argmax_\pi \mathbb{E}_{p_\mathcal{R},p_\mathbb{T}, \pi} \left[ \sum^T_{t=0}\gamma^tr_t \right] + \frac{1}{\beta} \mathcal{H}(\pi) \text{ s.t. } \mathbb{E}_{\tau \sim (p_\mathcal{T}, \pi), p_{\mathcal{c}_i}}[c_i(\tau)] \leq \epsilon_i ~{} \forall i \in [0,I]
$$
{% endkatexmm %}

where {% katexmm %}$c(\tau)${% endkatexmm %} is the tracjectory cost

depending on how {% katexmm %}$c(\tau)${% endkatexmm %} is defined, we can get more **restrictive** constraints than cumulative

for example, 

{% katexmm %}
$$
c(\tau) = 1 - \Pi_{(s,a)\in \tau } \phi (s,a) 
$$
{% endkatexmm %}

where {% katexmm %}$\phi(s,a)${% endkatexmm %} indicates the prob 

performing a under s is safe

stricter than additive cost

### **inverse constraint inference**

in practice

no constraints but expert demonstrations that follow constraints

goal

agent recover constraint from dataset

challenge

different reward & constraint combination

for identifiability

ICRL assumes ***rewards are observable***

goal

recover the **minimum constraint set** that best explains expert data

**key difference from IRL**

IRL learns **reward** from unconstrained MDP

**maximum entropy constrain inference**

{% katexmm %}
$$
p(\mathcal{D}_e | \phi) = \frac{1}{(Z_{\mathcal{M}^{\hat{c}_\phi}})^N} \Phi^N_{i=1} \exp [r(\tau^{i})] 
\mathbb{1}^{\mathcal{M}^{\hat{c}_\phi}}(\tau^{(i)})
$$
{% endkatexmm %}

where 

1. {% katexmm %}$N${% endkatexmm %} denotes # of trajectories in the demonstration dataset {% katexmm %}$\mathcal{D}_e${% endkatexmm %}
2. normalizing term {% katexmm %}$Z_{\mathcal{M}^{\hat{c}_\phi}} = \int \exp [r(\tau)] \mathbb{1}^{\mathcal{M}^{\hat{c}_\phi}} (\tau) \text{d} \tau${% endkatexmm %}
3. indicator {% katexmm %}$1^{\mathcal{M}^{\hat{c}_\phi}}(\tau^{(i)})${% endkatexmm %}. can be defined by {% katexmm %}$\phi(\tau^{(i)}) = \Pi^T_{t=1}  \phi_t${% endkatexmm %} and {% katexmm %}$\phi_t(s_t^i,a_t^i)${% endkatexmm %} defines to what extent the trajectory {% katexmm %}$\tau^{(i)}${% endkatexmm %}  is feasible

{% katexmm %}
$$
\nabla_\theta \log[p(\mathcal{D}_e|\phi)] = \sum^N_{i=1} [\nabla_\phi \sum^T_{t=0} \log\left[\phi_\theta(s_t^{(i)},a_t^{(i)})]\right] - N \mathbb{E}_{\hat{\tau}\sim \pi_{\mathcal{M}^\phi}}\left[ \nabla_\theta \sum^T_{t=0} \log[\phi_\theta(\hat{s}_t, \hat{a}_t] \right]
$$
{% endkatexmm %}

ICRL can be formulated as a **bi-level optimization problem** that iteratively updates

upper-level objective

policy optimization

lower-level objective

constraint learning until convergence 

{% katexmm %}$\pi${% endkatexmm %} matches expert policy