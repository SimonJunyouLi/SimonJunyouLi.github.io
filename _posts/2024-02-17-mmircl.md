---
title: Multi-Modal Inverse Constrained Reinforcement Learning from a Mixture of Demonstrations
subtitle: An algorithm for simultaneously estimating multiple constraints corresponding to different types of experts.
authors: G Qiao, G Liu, P Poupart, Z Xu
layout: default
date: 2024-02-17
keywords: machine learning, reinforcement learning, inverse reinforcement constraint learning
tags: ml rl ircl
published: true
---

## abstract

inverse constraint reinforcement learning (icrl)

recover the underlying constraints

existing algorithms assume single type of agent

challenging to explain why unified constraint function

**multi-modal inverse constrained reinforcement learning (MMICRL)**

different types of experts

**flow-based density estimator**

unsupervised expert identification 

novel multi-modal **constrained policy optimization**

minimizes agent-conditioned policy entropy

maximizes the unconditioned one

**contrastive learning framework** ← enhance robustness

capture diversity

### introduction

<div class='figure'>
    <img src="/image/mmicrl_arch.png"
         style="width: 60%; display: block; margin: 0 auto;"/>
</div>
<!-- ![Screenshot 2024-02-19 at 5.03.57 PM.png](paper%204%2091fdb7bed4cf442fa7734efa89dad3cf/Screenshot_2024-02-19_at_5.03.57_PM.png) -->

problematic to leverage one single constraint model

previously to differentiate expert demonstrations

inferred the latent structure of expert demonstrations

identified by behavioral preferences

no theoretical guarantee that optimal model is identifiable

contributions

1. **unsupervised agent identification**
    
    flow-based density estimator
    
    each agent’s policy must correspond to a unique occupancy measure
    
2. **agent-specific constraint inference**
    
    using the identified demonstrations
    
    estimates a permissibility function → construct constraints for each type of agent
    
3. **multi-modal policy optimization**
    
    comparing the similarity between
    
    expert trajectories
    
    generated ones by imitation policies under inferred constraints
    
    treat generated trajectories as
    
    noisy embeddings of agents
    
    ***enhance similarity between embeddings for agents of the same type***
    

## problem definition

**constrained mixture-agent markov decision process**

CMA-MDP {% katexmm %}$\mathcal{M}^\phi${% endkatexmm %}

{% katexmm %}
$$
(\mathcal{S}, \mathcal{A}, \mathcal{Z}, \text{R}, p_\mathcal{\tau}, \{(p_{\mathcal{c}_i}, \epsilon_i) \}_{\forall i}, \gamma, \mu_0)
$$
{% endkatexmm %}

{% katexmm %}$\mathcal{Z}${% endkatexmm %} denotes the latent code (for specifying expert agents)

in contrast to multi-agent reinforcement learning (MARL)

multiple agents act concurrently

do not distinguish agent types, policies, and constraints

cma-mdp allows

only one agent to operate at a given time

**policy update under conditional constraints**

constrained reinforcement learning (CRL)

goal → max reward under conditional constraints

{% katexmm %}
$$
\mathcal{J}(\pi | z) = \max_\pi \mathbb{E}_{\mu_0, p_\tau, \pi} \left[ \sum^T_{t=0} \gamma^tr_t \right] + \beta \mathcal{H}(\pi) \\
\text{ s.t. } \mathbb{E}_{\tau \sim (\mu_o, p_\tau, \pi), p_{\mathcal{c}_j}}\left[ c_j(\tau|z)\right] \leq \epsilon_j(z) ~{} \forall j \in [0, J]
$$
{% endkatexmm %}

where

{% katexmm %}$\mathcal{H} (\pi)${% endkatexmm %} denotes policy entropy weighted by {% katexmm %}$\beta${% endkatexmm %}

{% katexmm %}$c(\tau | z) = 1 - \Pi_(s,a)\in \tau \phi(s,a|z)${% endkatexmm %} denotes trajectory cost

{% katexmm %}$\phi(s,a|z)${% endkatexmm %} denotes permissibility function that indicates the prob. performing action {% katexmm %}$a${% endkatexmm %} under state {% katexmm %}$s${% endkatexmm %} is safe for agent {% katexmm %}$z${% endkatexmm %}

assumes constraints are known

instead, we only have

expert demonstrations that follow the underlying constraints

therefore, agents must recover the constraints

**constraint inference from a mixture of expert dataset**

{% katexmm %}
$$
p(\mathcal{D}_E|\phi) = \frac{1}{Z_{\mathcal{M}^{\hat{c}_{\phi}}}} \Pi^N_{i=1} \sum_z \mathbb{1}_{\tau^i \in \mathcal{D}_z} \exp [r(\tau^{(i)})] 1^{\mathcal{M}^{\hat{c}_\phi}}(\tau^{(i)}|z)
$$
{% endkatexmm %}

where 

{% katexmm %}$N${% endkatexmm %} denotes # of trajectories in the demonstration dataset

{% katexmm %}$Z_{\mathcal{M}^{\hat{c}_\phi}}${% endkatexmm %} denotes a normalizing term

{% katexmm %}$1^{\mathcal{M}^{\hat{c}_\phi}}(\tau^{(i)})${% endkatexmm %} denotes permissibility indicator and can be defined by 

{% katexmm %}$\phi(\tau^{(i)}|z) = \Pi^T_{t=1} \phi_t(s^i_t,a^i_t | z)${% endkatexmm %}

{% katexmm %}$1_{\tau^{(i)}\in \mathcal{D}_z}${% endkatexmm %} denotes whether the trajectory {% katexmm %}$\tau^{(i)}${% endkatexmm %} is generated by the agent {% katexmm %}$z${% endkatexmm %} (agent identifier)

 

## icrl for a mixture of experts

mmicrl

1. exhibit high entropy when agent type is unknown
2. collapse to a specific behavioral mode when type is determined
    
    low entry when agent type is known
    
{% katexmm %}
$$
\text{Minimize} ~{}~{} -\alpha_1\mathcal{H}[\pi(\tau)] + \alpha_2 \mathcal{H}[\pi(\tau|z)] \\
\text{Subject to} ~{} 
\begin{aligned}
&\int\pi(\tau|z)f_z(\tau)\text{d}\tau = \frac{1}{N} \sum_{\tau\in \mathcal{D}_z}f(\tau) \\ 
&\int \pi(\tau|z)\text{d}\tau = 1 \\
& \int \pi(\tau|z)\log \phi(\tau|z)\text{d}\tau \geq \epsilon
\end{aligned}
$$
{% endkatexmm %}

where 

{% katexmm %}$f(\cdot)${% endkatexmm %} denotes a latent feature extractor

objective differs from traditional maximum entropy rl in two ways

1. minimizes an entropy conditioned on agent types
2. additional constraint related to policy permissibilty

**Proposition** *LET {% katexmm %}$p(z|\tau)${% endkatexmm %} denote the trajectory-level agent identifier, let {% katexmm %}$r(\tau) = \frac{\lambda_0}{\alpha_2 - \alpha_1} f(\tau)${% endkatexmm %} denote the trajectory rewards, let {% katexmm %}$Z_{\mathcal{M}^{\hat{c}_\phi}}${% endkatexmm %} denote a normalizing term. The optimal policy of the above optimization problem can be represented as*

{% katexmm %}
$$
\pi(\tau|z) = \frac{1}{Z_{\mathcal{M}^{\hat{c}_\phi}}} \exp \left[ \frac{\alpha_1 \mathbb{E}_{z \sim p(z)} [\log(p(z | \tau))]}{\alpha_2 - \alpha_1} + r(\tau) \right] \phi(\tau | z)^{\frac{\lambda_2}{\alpha_1 - \alpha_2}}
$$
{% endkatexmm %}

kep **iterative** steps of mmicrl

1. unsupervised agent identification for calculating {% katexmm %}$p(z|\tau)${% endkatexmm %}
2. conditional inverse constraint inference for deducing {% katexmm %}$\phi(\tau | z)${% endkatexmm %}
3. multi-modal policy update for approximating {% katexmm %}$\pi(\tau|z)${% endkatexmm %}

mmicrl **alternates** between these steps until 

imitation policies reproduce expert trajectories

***question: what if expert trajectories violates the underlying constraints as well?***

### unsupervised agent identification

mmicrl performs

trajectory level agent identification

***state action density***

{% katexmm %}$\rho_\pi(s,a) = (1 - \gamma)\pi(a|s) \sum^\infty_{t=0} \gamma^t p(s_t = s | \pi)$ where $p(s_t = s | \pi)${% endkatexmm %}  is the probability density of state $s$ at time step $t$ following policy {% katexmm %}$\pi${% endkatexmm %}

**Proposition 4.2.** Suppose {% katexmm %}$\rho${% endkatexmm %} is the occupancy measure tha satisfies the ***Bellman flow constraints: {% katexmm %}$\sum_a \rho(s,a) = \mu_0(s) + \gamma \sum_{s^\prime,a} \rho(s^\prime,a) P_\tau (s^\prime|s,a)${% endkatexmm %} and {% katexmm %}$\rho(s,a) \geq 0${% endkatexmm %}. Let the policy defined by {% katexmm %}$\pi_\rho (a | s) = \frac{\rho(s,a)}{\int \rho(s,a^\prime)\text{d}a^\prime}${% endkatexmm %}, then {% katexmm %}$\pi_\rho${% endkatexmm %} is the only policy whose occupancy measure is {% katexmm %}$\rho${% endkatexmm %}.***

***Bellman Flow Constraint** is defined as* 

{% katexmm %}
$$
\mathcal{\chi}(s,a) = \pi(a|s) \cdot [(1 - \gamma) \mu_0(s) + \gamma \int \chi(s^\prime, a^\prime) \tau(s|s^\prime, a^\prime)\text{d}s^\prime d^\prime]; \quad \chi(s,a) \geq 0
$$
{% endkatexmm %}

*It can be shown that occupancy measure {% katexmm %}$\rho^\pi(s,a)${% endkatexmm %} is a **unique solution** to Bellman flow constraint*

**Density Estimation**

“one can identify an expert agent by examining the occupancy measures in the expert trajectories”

*conditional Flow-based Density Estimator (CFDE)*

extimates the density of input variables in the training data distribution under an auto-regressive constraint

conditions on agent types 

the function {% katexmm %}$\psi${% endkatexmm %} is implemented by stacking multiple MADE layers

{% katexmm %}
$$
p(x_i | x_{1:i-1}, z) n= \mathcal{N}(x_i|\mu_i, (\exp(\alpha_i))^2) \\ \text{ where } \mu_i = \psi_{\mu_i} (x_{1:i-1}, z) \text{ and } \alpha_i = \psi_{\alpha_i} (x_{1:i-1}, z) \\ \\
$$
{% endkatexmm %}

{% katexmm %}
$$
p_\psi(z|\tau) = \frac{\Pi_{(s,a)\in\tau}p_\psi(s,a|z)}{\sum_{z^\prime} \Pi_{(s,a)\in\tau} p_\psi(s,a|z^\prime)}
$$
{% endkatexmm %}

**Agent Identification**

add {% katexmm %}$\tau^i$ to $\mathcal{D}_z${% endkatexmm %} if {% katexmm %}$z = \argmax_z \sum_{(s,a)\in\tau_i} \log[p_\psi(s,a|z)]${% endkatexmm %}

### agent-specific constraint inference

{% katexmm %}
$$
\nabla_\omega\log[p(\mathcal{D}_z|\phi,z)] \coloneqq \sum^N_{i=1} [\nabla_\phi \sum^T{t=0} \eta \log[\phi_\omega (s^{(i)}_t, a^{(i)}_t |z)]] - N \mathbb{E}_{\hat{\tau}\sim\pi_{\mathcal{M}^\phi}(\cdot|z)} [\nabla_\phi \sum^T_{t=0}[\eta \log [\phi_\omega \hat{s}_t, \hat{a}_t |z)]]
$$
{% endkatexmm %}

### multi-modal policy optimization

the multi-modal policy optimization object 

{% katexmm %}
$$
\min_\pi -\mathbb{E}_{\pi(\cdot|z)} \left[ \sum^T_{t=1} m\gamma^t r(s_t, a_t) \right] - \alpha_1 \mathcal{H}[\pi(\tau)] + \alpha_2 \mathcal{H}[\pi(\tau|z)] \\
\text{s.t. } \mathbb{E}_{\pi(\cdot|z)} \left( \sum^h_{t=0} \gamma^t \log \phi_\omega(s,a,z)\right) \geq \epsilon
$$
{% endkatexmm %}

**learning diverse policies via contrastive estimation**

directly augmenting rewards lead to sub-optimal policy

a large penalty to trajectories with log {% katexmm %}$p_\psi(z|\tau)${% endkatexmm %}

more sensitive to dense estimation rather than reward signals 

replace the identification probability with a contrastive estimation method

{% katexmm %}
$$
\min_\pi -\mathbb{E}_{\pi(\cdot|z)} \left( r(\tau) + \alpha_1 L_{ce} (\tau, \mathcal{V}_{1, \dots, |Z|} )  \right)+ (\alpha_2 - \alpha_1) \mathcal{H}(\pi(\tau|z)) \\ \text{ s.t. } \mathbb{E}_{\pi(\cdot|z)} \left( \sum^h_{t=0} \gamma^t \log \phi_\omega (s,a,z)\right) \geq\epsilon
$$
{% endkatexmm %}

where {% katexmm %}$\mathcal{Z}${% endkatexmm %} is the probing sets 

using sets generated by CFDE

{% katexmm %}
$$
L_{ce}(\tau, \mathcal{V}_{1, \dots, |Z|} ) = \sum^T_{t=0} \log \frac{\exp \left[ \sum_{(\hat{s}_z, \hat{a}_z) \in \mathcal{V}_z} f_s[(s_t, a_t), (\hat{s}_t, \hat{a}_t)] \right]}{\sum_{\tilde{z} \in \mathcal{Z}} \exp \left[ \sum_{(\hat{a}, \hat{a}) \in \mathcal{V}_{\tilde{z}}} f_s[(s_t,a_t),(\hat{s},\hat{a})]\right]}
$$
{% endkatexmm %}

where $f_s$ denotes the score function for measuring the similarity between different state-action pairs using cosine similarity

{% katexmm %}$(s,a) \in \{\tau, \mathcal{V}_z\}${% endkatexmm %} → positive embeddings for {% katexmm %}$\pi(\cdot | z)${% endkatexmm %}

{% katexmm %}$(\tilde{s}, \tilde{a}) \in \{\mathcal{V}_{\tilde{z}}\}_{\tilde{z} \neq z}${% endkatexmm %} → negative embeddings for {% katexmm %}$\pi(\cdot | z)${% endkatexmm %}

consider generation as injecting environmental noise into the policy

since {% katexmm %}$\mathcal{V}_z${% endkatexmm %} belongs to high-conditional density region in {% katexmm %}$p(\cdot|z)${% endkatexmm %}

knowledge from density estimator is integrated into policy updates