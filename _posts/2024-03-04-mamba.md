---
title: Mamba - Linear-Time Sequence Modeling with Selective State Spaces
subtitle: My summary notes of the Mamba paper. 
authors: Albert Gu, Tri Dao
layout: default
date: 2024-03-04
keywords: machine learning
tags: ml
published: true
---

Foundation models are large models pretrained on massive data then adapted for downstream tasks. FMs often employ sequence models as backbone and is currently predonminantly based on the Transformer.

The efficacy of **self-attention** in the Transformer attributes to
- the ability to route information densely within a *context window*

However, this property also brings in foundamental drawbacks
- the inability to model anything outside of the *finite* context window
- *quadratic scaling* with respect to the window length

Recently, *structured state space sequence models* (SSMs) were proposed as a new class of architectures for sequence modeling. These models can be interpreted as a combination of *RNN*s and *CNN*s, with inspirtaion from *classical state space models*. The obtained results from SSMs are very promising, as it not only scale linearly or near-linearly with respect to the sequence length but also demonstrates outstanding ability in modelling long-range dependencies, dominating benchmarks such as the Long Rnange area. 









previous ssm
    lti (linear time invariant)
        does not update the matrices
    this paper makes it variant such that it performs better while being efficient

    is time invariant related input dependent?

    scanning?
    can train as a audio captioning model?